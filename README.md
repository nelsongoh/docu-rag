# Document Retrieval-Augmented Generation (Docu-Rag)
This is my pet project to learn to build a retrieval-augmented generation for building a question-and-answer system based on some input documents. This project was built with the help of Claude Code, which greatly accelerated my learning journey through interactive development and code review.

## Overview
I'll be building a system that answers questions about payment API integration, using retrieval-augmented generation. I'll be using Stripe's documentation as reference to build the available knowledge base as it's very well structured.

### RAG Pipeline
In the RAG pipeline, this is how the flow would look like:
1. A user's question is submitted through the app
2. The question is broken down and converted into a vector / document embeddings
3. The embeddings are looked up against the vector database for similar chunks
4. The top X most relevant chunks are then returned
5. The relevant chunks, along with the original user question, will be turned into a prompt
6. The prompt is then sent to Claude
7. Claude generates an answer, based on the provided prompt. The relevant citations are then provided.

## Libraries used in the project

| Library | Purpose | Reason for using it |
| ------- | ------- | ------------------- |
| anthropic | To use the Claude API client | To call Claude's models for generating responses |
| openai | To use the OpenAI API client | To generate embeddings / vectors for the documented knowledge sources to be used in the RAG |
| chromadb | Vector database | To store and look up document embeddings. Selected because of its simple, local-first ability. |
| streamlit | A web UI framework | To create an interactive demo interface easily using Python |
| beautifulsoup4 | HTML parser | To extract text from documentation websites |
| requests | HTTP request library | To simplify HTTP request making, useful in performing methods such as GET requests to fetch web pages in the text extraction |
| python-dotenv | Environment variable library | To manage fetching the environment variables in our project |
| tiktoken | Token counting library | To help us count tokens and manage project costs |

## Approach
### 1. Knowledge Collection
Scrape the source(s) of knowledge (just once, or infrequently from time-to-time) from web pages to use to augment the model's knowledge, using libraries such as BeautifulSoup. Scrapers will implement a rate limiter to prevent overwhelming the web pages.

### 2. Document Chunking
Breaks the large documents that were downloaded into smaller "chunks" that can be embedded as vectors to be searched. These chunks should be split by semantic boundaries, to ensure that the sentences make semantic sense when it needs to be looked up. Chunks should also contain overlap between chunks in order to preserve continuation of context. These chunks should also include metadata in order to help provide source information for the model to help perform citations.

### 3. Knowledge Storing
The chunks which were generated will then be stored in the vector database. Helper methods to perform the storing and retrieval of information from the database will also be developed at this stage.

### 4. Building the RAG pipeline
At this stage is where we piece the different steps together. Here we create a method to accept a user's query input, and to embed the query into a vector. This query vector is then looked up against the knowledge which was stored in the vector database to fetch the X relevant chunks, relative to the user's query. A prompt with:

1. instructions on what the model should do, 
2. the relevant chunks that were fetched from the vector database
3. the original user's question

are then passed to the model. Parameters such as temperature can also be set at this stage.

### 5. Evaluation
Now that we have the pipeline, we need to test to evaluate the effectiveness of our pipeline. Here we create a test dataset along with a test script. The test dataset will contain the question, the expected topics that it should cover, and the category it should match. In the evaluation test script, we will create an evaluation prompt which provides instructions to the model to evaluate the responses provided by an LLM model, along with the original question, and the responses generated by the LLM. With the evaluation, we should then save it out for further analysis.

### 6. Evaluation Analysis
In the analysis stage, we look back at the saved evaluation results and look for areas of improvement for the model. Here, we can also detect for potential hallucinations provided in the answers, and look into it further.

### 7. Productionizing
At this stage we can start to look at productionizing the code to harden it, by introducing features to handle errors, retries, and rate limiting to help our systems fail gracefully and avoid overwhelming the infrastructure put in place. We can also add cost tracking to help monitor and track our costs incurred from the use of this project, while visualizing it via a monitoring dashboard. This dashboard can be showcased via the Streamlit library.

## Project structure
```
docu-rag/
├── .env                    # API keys, though not best practice to store them in here. Where possible, should be managed using a secrets manager to fetch these sensitive credentials.
├── .gitignore
├── README.md
├── requirements.txt
├── data/
│   ├── raw/               # Original docs
│   └── processed/         # Chunked docs
├── docs/
│   ├── concepts.md        # Documentation of the concepts encountered during the build of this pet project.
├── src/
│   ├── __init__.py
│   ├── data_processor.py  # Document chunking
│   ├── retriever.py       # Retrieval logic
│   ├── rag_chain.py       # Main RAG pipeline
│   └── evaluator.py       # Evaluation framework
├── app.py                 # Streamlit UI
├── eval/
│   ├── test_questions.json
│   └── evaluation_results.json
└── notebooks/
    └── exploration.ipynb  # For experimentation
```

## Knowledge Collection & Building
### Adding Knowledge Sources
To add your own knowledge sources, add an entry to the `/config/doc_sources.json`. You can use your own `name` within the entry, but please ensure you de-conflict it from the other entries within the list of sources.

To enable / disable sources to collect knowledge from, please set the `enabled` field within the dictionary in the `sources` list to `true` if you'd like to fetch the source. Otherwise if it should be skipped, set it to `false`. For example:

```
"sources": [
    {
      "name": "stripe",
      "enabled": true,
      ...
    }
]
```

### Configuring the Source Collector
Each source should have a collector (which houses the logic needed to process the inputs from the documentation collected). For example, to process Stripe's documentation, a `StripeDocCollector` has been defined in `/src/collectors/__init__.py` in the `COLLECTORS` variable. The key in that dictionary corresponds to the name that was set in the `doc_sources.json` file. For example:

```
COLLECTORS = {
    'stripe': StripeDocCollector,
}
```

### Running the Document Collection
Once the setup is complete, run `python src/data_collector.py` from the root of the project directory.

## Document Chunking
**PRE-REQUISITE**: Please ensure that you have the raw knowledge saved first before you attempt to chunk the documents (otherwise you'll have nothing to chunk!).

To process the raw documents that were saved in the _Knowledge Collection & Building_ section, run `python src/data_processor.py`.

## Knowledge Vectorization & Storage
**PRE-REQUISITE**: Please ensure that you have your processed knowledge chunks as completed in the _Document Chunking_ section.

### Vectorization Configuration
To set up the vectorization configuration, please add an entry in the `/config/vector_store_config.json`. You will first need to add a dictionary entry in the `collections` list to be stored in the vector database. This contains the following metadata:

| Dictionary Key | Dictionary Value Description |
| -------------- | ---------------------------- |
| `name` | The name of the collection |
| `source` | This is the name based on the `doc_sources.json` file, and should be the same name in the `/data/processed/chunked_<name>_docs.json` |
| `description` | The description of what this document embedding collection is storing |
| `metadata.domain` | The domain of knowledge this documentation is in. For example, Stripe's API documentation lies in the "payments" realm, hence its domain is marked as such. |
| `metadata.provider` | This is the provider of the documentation |
| `metadata.version` | The version of the documentation |
| `metadata.content_type` | The type of the documentation provided. For example, if it is API related documentation, it can say "api_documentation" |

### Running the Vectorization and Storage Process

**PRE-REQUISITES**:
1. You must have successfully run `python src/data_collector.py` to collect raw documentation
2. You must have successfully run `python src/data_processor.py` to chunk the raw documents
3. You must have configured the vector store collections in `/config/vector_store_config.json`

Once these prerequisites are met, run the following command to start vectorizing and storing the chunked documents:

```bash
python scripts/init_vector_store.py
```

You can also customize the paths if needed:

```bash
python scripts/init_vector_store.py --config config/vector_store_config.json --data data/processed --db ./chroma_db
```

#### What Happens During Vectorization

1. **Configuration Loading**: The script loads your vector store configuration to understand which collections to create
2. **Chunk Loading**: All processed chunks from the data directory are loaded into memory
3. **Cost Estimation**: A cost estimate is displayed showing:
   - Total number of documents to vectorize
   - Embedding model being used
   - Estimated cost in USD based on OpenAI pricing
   - Pricing information date
4. **User Confirmation**: You will be asked to confirm with a `yes`/`no` input before proceeding. This is a guardrail to prevent accidental vectorization as this will incur a cost.
5. **Vectorization**: Once confirmed, documents are vectorized and added to ChromaDB collections
6. **Summary**: A summary report is displayed showing successful, failed, and skipped collections

#### Options

- `--config CONFIG_PATH`: Path to vector_store_config.json (default: config/vector_store_config.json)
- `--data DATA_DIR`: Directory containing processed chunks (default: data/processed)
- `--db DB_PATH`: ChromaDB storage path (default: ./chroma_db)

#### Troubleshooting

If you encounter errors during vectorization:

1. **"No processed chunks found"**: Make sure you've run `python src/data_processor.py` first
2. **"Configuration file not found"**: Make sure you've created `/config/vector_store_config.json`
3. **"OpenAI API key not found"**: Make sure your `.env` file contains `OPENAI_API_KEY`
4. **Rate limit errors**: The system will automatically retry with exponential backoff, but you may need to wait before running again

#### Resuming Interrupted Vectorization

If vectorization is interrupted (e.g., due to network issues), you can safely run the command again. The system will automatically skip chunks that have already been vectorized and continue from where it left off.

## Interacting with the RAG Model via Web Application

### Overview
The Streamlit web application provides an interactive interface to query the RAG model. It allows you to ask questions about the documentation sources that have been vectorized and stored. The application provides real-time cost estimation and displays relevant source citations for each answer.

### Pre-requisites
Before you can run the web application, ensure that you have completed the following steps:

1. Successfully collected raw documentation via `python src/data_collector.py`
2. Successfully processed and chunked the documents via `python src/data_processor.py`
3. Successfully vectorized and stored the chunks via `python scripts/init_vector_store.py`
4. Configured your documentation sources in `/config/doc_sources.json`
5. Configured your vector store collections in `/config/vector_store_config.json`

### Running the Web Application

To start the Streamlit web application, run the following command from the root of the project directory:

```bash
streamlit run app.py
```

The application will start and open in your default web browser at `http://localhost:8501`. If it doesn't open automatically, you can navigate to this URL manually.

### Using the Web Application

#### Selecting Documentation

At the top of the sidebar, you will see a dropdown menu labeled **"Choose documentation to query"**. This allows you to select which documentation source you'd like to query against. The dropdown options are dynamically loaded from your `/config/vector_store_config.json` file, so you can add multiple documentation sources and switch between them seamlessly.

#### Asking Questions

Once you've selected a documentation source, you can type your question into the chat input box at the bottom of the screen. For example, you might ask "How do I handle failed payment retries?" for the Stripe documentation.

#### Cost Estimation & Confirmation

After you submit a question, the application will:

1. **Estimate the cost**: The system performs a cost estimate to show you how much the query will cost (based on the selected model's pricing). This includes:
   - Estimated input tokens (the tokens in your question + retrieved context)
   - Estimated output tokens (approximate response length)
   - Estimated cost in USD

2. **Request confirmation**: You will see two buttons:
   - **"✅ Yes, generate response"**: Proceed with generating the response
   - **"❌ No, cancel"**: Cancel the operation and return to the chat input

This confirmation step is a guardrail to prevent unexpected API costs.

#### Viewing Results

Once you confirm by clicking "Yes", the application will:

1. Retrieve relevant context from the vector database
2. Generate a response using Claude based on the retrieved context
3. Display the generated answer
4. Show the actual token usage and cost (compared to the estimate)
5. Display source citations in an expandable section

Each response includes:

- **The answer**: A comprehensive response to your question, grounded in the retrieved documentation
- **View Sources**: An expandable section showing the specific documentation chunks that were used to generate the answer
- **Token usage**: The actual number of input and output tokens used for the query
- **Actual cost**: The actual cost incurred for this query

#### Example Questions

The sidebar provides example questions that you can click to quickly test the system. These are pre-populated for the Stripe documentation and include:

- "How do I handle failed payment retries?"
- "What's the difference between Payment Intent and Charge?"
- "How do webhooks work for subscriptions?"
- "What are common payment errors and how to handle them?"
- "How do I implement idempotency for payments?"

### Troubleshooting

If you encounter issues while using the web application:

1. **"No documentation sources available"**: Ensure you've configured `/config/doc_sources.json` and `/config/vector_store_config.json` correctly
2. **"No relevant context found"**: The question may not have matches in the vectorized documentation. Try rephrasing your question or ensure the documentation has been fully vectorized
3. **API key errors**: Ensure your `.env` file contains valid `ANTHROPIC_API_KEY` for Claude responses
4. **Connection errors**: Make sure you have a stable internet connection, as the application needs to communicate with the Anthropic API for generating responses

### Customizing the Application

The web application behavior can be customized by modifying values in the RAG initialization section of `app.py`. You can change:

- `collection_name`: The vector store collection to query against
- `model`: The Claude model to use for generating responses
- `prompt_preset`: The prompt template to use

For advanced configuration, see the documentation in `config/rag_prompts.json` for prompt templates and `config/constants.py` for model selections and pricing information.