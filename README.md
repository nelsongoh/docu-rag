# Document Retrieval-Augmented Generation (Docu-Rag)
This is my pet project to learn to build a retrieval-augmented generation for building a question-and-answer system based on some input documents.

## Overview
I'll be building a system that answers questions about payment API integration, using retrieval-augmented generation. I'll be using Stripe's documentation as reference to build the available knowledge base as it's very well structured.

### RAG Pipeline
In the RAG pipeline, this is how the flow would look like:
1. A user's question is submitted through the app
2. The question is broken down and converted into a vector / document embeddings
3. The embeddings are looked up against the vector database for similar chunks
4. The top X most relevant chunks are then returned
5. The relevant chunks, along with the original user question, will be turned into a prompt
6. The prompt is then sent to Claude
7. Claude generates an answer, based on the provided prompt. The relevant citations are then provided.

## Libraries used in the project

| Library | Purpose | Reason for using it |
| ------- | ------- | ------------------- |
| anthropic | To use the Claude API client | To call Claude's models for generating responses |
| openai | To use the OpenAI API client | To generate embeddings / vectors for the documented knowledge sources to be used in the RAG |
| chromadb | Vector database | To store and look up document embeddings. Selected because of its simple, local-first ability. |
| streamlit | A web UI framework | To create an interactive demo interface easily using Python |
| beautifulsoup4 | HTML parser | To extract text from documentation websites |
| requests | HTTP request library | To simplify HTTP request making, useful in performing methods such as GET requests to fetch web pages in the text extraction |
| python-dotenv | Environment variable library | To manage fetching the environment variables in our project |
| tiktoken | Token counting library | To help us count tokens and manage project costs |

## Approach
### 1. Knowledge Collection
Scrape the source(s) of knowledge (just once, or infrequently from time-to-time) from web pages to use to augment the model's knowledge, using libraries such as BeautifulSoup. Scrapers will implement a rate limiter to prevent overwhelming the web pages.

### 2. Document Chunking
Breaks the large documents that were downloaded into smaller "chunks" that can be embedded as vectors to be searched. These chunks should be split by semantic boundaries, to ensure that the sentences make semantic sense when it needs to be looked up. Chunks should also contain overlap between chunks in order to preserve continuation of context. These chunks should also include metadata in order to help provide source information for the model to help perform citations.

### 3. Knowledge Storing
The chunks which were generated will then be stored in the vector database. Helper methods to perform the storing and retrieval of information from the database will also be developed at this stage.

### 4. Building the RAG pipeline
At this stage is where we piece the different steps together. Here we create a method to accept a user's query input, and to embed the query into a vector. This query vector is then looked up against the knowledge which was stored in the vector database to fetch the X relevant chunks, relative to the user's query. A prompt with:

1. instructions on what the model should do, 
2. the relevant chunks that were fetched from the vector database
3. the original user's question

are then passed to the model. Parameters such as temperature can also be set at this stage.

### 5. Evaluation
Now that we have the pipeline, we need to test to evaluate the effectiveness of our pipeline. Here we create a test dataset along with a test script. The test dataset will contain the question, the expected topics that it should cover, and the category it should match. In the evaluation test script, we will create an evaluation prompt which provides instructions to the model to evaluate the responses provided by an LLM model, along with the original question, and the responses generated by the LLM. With the evaluation, we should then save it out for further analysis.

### 6. Evaluation Analysis
In the analysis stage, we look back at the saved evaluation results and look for areas of improvement for the model. Here, we can also detect for potential hallucinations provided in the answers, and look into it further.

### 7. Productionizing
At this stage we can start to look at productionizing the code to harden it, by introducing features to handle errors, retries, and rate limiting to help our systems fail gracefully and avoid overwhelming the infrastructure put in place. We can also add cost tracking to help monitor and track our costs incurred from the use of this project, while visualizing it via a monitoring dashboard. This dashboard can be showcased via the Streamlit library.

## Project structure
```
docu-rag/
├── .env                    # API keys, though not best practice to store them in here. Where possible, should be managed using a secrets manager to fetch these sensitive credentials.
├── .gitignore
├── README.md
├── requirements.txt
├── data/
│   ├── raw/               # Original docs
│   └── processed/         # Chunked docs
├── docs/
│   ├── concepts.md        # Documentation of the concepts encountered during the build of this pet project.
├── src/
│   ├── __init__.py
│   ├── data_processor.py  # Document chunking
│   ├── retriever.py       # Retrieval logic
│   ├── rag_chain.py       # Main RAG pipeline
│   └── evaluator.py       # Evaluation framework
├── app.py                 # Streamlit UI
├── eval/
│   ├── test_questions.json
│   └── evaluation_results.json
└── notebooks/
    └── exploration.ipynb  # For experimentation
```

## Knowledge Collection & Building
### Adding Knowledge Sources
To add your own knowledge sources, add an entry to the `/config/doc_sources.json`. You can use your own `name` within the entry, but please ensure you de-conflict it from the other entries within the list of sources.

To enable / disable sources to collect knowledge from, please set the `enabled` field within the dictionary in the `sources` list to `true` if you'd like to fetch the source. Otherwise if it should be skipped, set it to `false`. For example:

```
"sources": [
    {
      "name": "stripe",
      "enabled": true,
      ...
    }
]
```

### Configuring the Source Collector
Each source should have a collector (which houses the logic needed to process the inputs from the documentation collected). For example, to process Stripe's documentation, a `StripeDocCollector` has been defined in `/src/collectors/__init__.py` in the `COLLECTORS` variable. The key in that dictionary corresponds to the name that was set in the `doc_sources.json` file. For example:

```
COLLECTORS = {
    'stripe': StripeDocCollector,
}
```

### Running the Document Collection
Once the setup is complete, run `python src/data_collector.py` from the root of the project directory.

## Document Chunking
**PRE-REQUISITE**: Please ensure that you have the raw knowledge saved first before you attempt to chunk the documents (otherwise you'll have nothing to chunk!).

To process the raw documents that were saved in the _Knowledge Collection & Building_ section, run `python src/data_processor.py`.

## Knowledge Vectorization & Storage
**PRE-REQUISITE**: Please ensure that you have your processed knowledge chunks as completed in the _Document Chunking_ section.

### Vectorization Configuration
To set up the vectorization configuration, please add an entry in the `/config/vector_store_config.json`. You will first need to add a dictionary entry in the `collections` list to be stored in the vector database. This contains the following metadata:

| Dictionary Key | Dictionary Value Description |
| -------------- | ---------------------------- |
| `name` | The name of the collection |
| `source` | This is the name based on the `doc_sources.json` file, and should be the same name in the `/data/processed/chunked_<name>_docs.json` |
| `description` | The description of what this document embedding collection is storing |
| `metadata.domain` | The domain of knowledge this documentation is in. For example, Stripe's API documentation lies in the "payments" realm, hence its domain is marked as such. |
| `metadata.provider` | This is the provider of the documentation |
| `metadata.version` | The version of the documentation |
| `metadata.content_type` | The type of the documentation provided. For example, if it is API related documentation, it can say "api_documentation" |

### Running the Vectorization and Storage Process

**PRE-REQUISITES**:
1. You must have successfully run `python src/data_collector.py` to collect raw documentation
2. You must have successfully run `python src/data_processor.py` to chunk the raw documents
3. You must have configured the vector store collections in `/config/vector_store_config.json`

Once these prerequisites are met, run the following command to start vectorizing and storing the chunked documents:

```bash
python scripts/init_vector_store.py
```

You can also customize the paths if needed:

```bash
python scripts/init_vector_store.py --config config/vector_store_config.json --data data/processed --db ./chroma_db
```

#### What Happens During Vectorization

1. **Configuration Loading**: The script loads your vector store configuration to understand which collections to create
2. **Chunk Loading**: All processed chunks from the data directory are loaded into memory
3. **Cost Estimation**: A cost estimate is displayed showing:
   - Total number of documents to vectorize
   - Embedding model being used
   - Estimated cost in USD based on OpenAI pricing
   - Pricing information date
4. **User Confirmation**: You will be asked to confirm with a `yes`/`no` input before proceeding. This is a guardrail to prevent accidental vectorization as this will incur a cost.
5. **Vectorization**: Once confirmed, documents are vectorized and added to ChromaDB collections
6. **Summary**: A summary report is displayed showing successful, failed, and skipped collections

#### Options

- `--config CONFIG_PATH`: Path to vector_store_config.json (default: config/vector_store_config.json)
- `--data DATA_DIR`: Directory containing processed chunks (default: data/processed)
- `--db DB_PATH`: ChromaDB storage path (default: ./chroma_db)

#### Troubleshooting

If you encounter errors during vectorization:

1. **"No processed chunks found"**: Make sure you've run `python src/data_processor.py` first
2. **"Configuration file not found"**: Make sure you've created `/config/vector_store_config.json`
3. **"OpenAI API key not found"**: Make sure your `.env` file contains `OPENAI_API_KEY`
4. **Rate limit errors**: The system will automatically retry with exponential backoff, but you may need to wait before running again

#### Resuming Interrupted Vectorization

If vectorization is interrupted (e.g., due to network issues), you can safely run the command again. The system will automatically skip chunks that have already been vectorized and continue from where it left off.